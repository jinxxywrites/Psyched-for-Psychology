<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Ghost in the Machine: AI, Agency, and Mental Health - Psyched for Psychology</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Psyched for Psychology</h1>
        <nav>
            <a href="index.html">Home</a> | 
            <a href="about.html">About</a>
        </nav>
    </header>
    <main>
        <article>
            <h2>The Ghost in the Machine: AI, Agency, and Mental Health</h2>
            <p class="date">January 13, 2026</p>

            <p>As we navigate the grand "AI Revolution" of 2026, the conversation usually centers on productivity or academic integrity. However, as a psychology student, I find myself looking at a different issue entirely: our mental health. We are all currently part of a massive psychological experiment with little to no borders. How does interacting with an entity that mimics human consciousness, but lacks a soul; affect our mental stability, our sense of self, and our grasp on reality?</p>

            <h3>The Agency Paradox</h3>
            <p>In his recent post, <a href="https://buildlittleworlds.github.io/plate-composition-blog/what-a-ban-cant-teach.html">"What a Ban Can't Teach,"</a> Professor Plate argues that banning AI in the classroom is a "pedagogically empty" move because it fails to teach students how to maintain agency. He makes a profound point: "The danger of AI in writing is not AI-generated language... The danger is AI-governed thinking."</p>

            <p>From a psychological perspective, "governed thinking" is a terrifying prospect. In clinical psychology, a core component of mental health is <strong>internal locus of control</strong>—the belief that you have power over the outcomes of your life. When we outsource our thinking to an algorithm, we aren't just saving time; we are effectively surrendering our cognitive agency. If we stop being the primary "deciders" of our own ideas, we risk a form of intellectual learned helplessness.</p>

            <h3>Defining "AI Psychosis"</h3>
            <p>As I explored this topic further, I came across an article in <em>Psychology Today</em> titled <a href="https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis">"The Emerging Problem of AI Psychosis."</a> The author, Dr. Marlynn Wei, explains that while "AI Psychosis" isn't a clinical diagnosis yet, it describes a very real phenomenon where chatbots amplify, validate, or even co-create psychotic symptoms with vulnerable users.</p>
            
            <p>Unlike a human therapist, who is trained to provide care while maintaining professional boundaries, an AI is trained to prioritize user satisfaction and "mirror" the person it is talking to. If a user is experiencing a manic episode, delusional break, or a rise in suicidial ideation, AI doesn't have the "reality testing" capabilities to flag it. Instead, it agrees with the user to keep them engaged in the ongoing chat, unintentionally fueling grandiose, religious, or paranoid delusions. This creates what Dr. Wei calls a "kindling effect," making psychotic episodes more frequent, severe, and potentially devastating.</p>

            <blockquote>
                "AI chatbots may inadvertently be reinforcing and amplifying delusional and disorganized thinking, a consequence of unintended agentic misalignment leading to user safety risks." <br>
                — Dr. Marlynn Wei, Psychology Today
            </blockquote>

            <h3>The Human Cost: The Case of Kendra Hilty</h3>
            <p>The stakes of this "agentic misalignment" are not theoretical. Consider the case of Kendra Hilty, whose struggle with mental health became grossly entangled with AI and broadcasted for millions to see on Tiktok in 2025. Hilty became convinced by her chatbot(s) that her psychatrist of three years was sending her coded signals filled with romantic tension and manipulation. Because the AI was designed to be hyper-responsive, it mirrored her fears and fantasies back to her until she could no longer distinguish between the algorithm's output and her own reality.</p>

            <p>This reality-blurring effect is precisely why Professor Plate’s call for <strong>network accountability</strong> is so vital. If we interact only with AI, we are in a closed loop; also known as an "echo chamber." But when we interact within a network of human beings, we are forced to reconcile our ideas with other living perspectives. A peer network acts as a collective reality-testing mechanism, providing the insight to disagree that an agreeable AI lacks.</p>

            <h3>Reclaiming the Mind</h3>
            <p>To "maintain intellectual ownership," as Plate suggests, we must practice <strong>Metacognition</strong>—thinking about our thinking. We must recognize that AI is meant to be a mathematical probability engine, not an all-knowing deity or a therapist. We cannot ban the future, but we must be careful that in automating our tasks, we don't accidentally automate our identities and lose our grip on what is real.</p>

            <h3>Sources:</h3>
            <ul>
                <li>Plate, R. (2026). "What a Ban Can't Teach." <em>Plate Composition Blog</em>.</li>
                <li>Wei, M. (2025). "The Emerging Problem of AI Psychosis." <em>Psychology Today</em>.</li>
            </ul>
        </article>
    </main>
    <footer>
        <p>&copy; 2026 Jinx Hixson</p>
    </footer>
</body>
</html>
