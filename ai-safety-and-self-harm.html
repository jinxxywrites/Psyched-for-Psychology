<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theories of Safety: AI-Encouraged Self Injury- Psyched for Psychology</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Psyched for Psychology</h1>
        <nav>
            <a href="index.html">Home</a> | 
            <a href="about.html">About</a>
        </nav>
    </header>
    <main>
        <article>
            <h2>Theories of Safety: AI-Encouraged Self Injury</h2>
            <p class="date">January 20, 2026</p>

            <p>As we delve deeper into the intersection of psychology and technology, we often talk about "safety" as a technical feature— a set of guardrails or filters designed to keep a machine from getting information wrong. But as a psychology student, I want to take a look at what happens when a machine’s programming doesn’t match the human reality of a mental health crisis. The stakes of this question are no longer theoretical; they're life and death.</p>

            <h3>"Forever 16"- The Case of Adam Raine</h3>
            <p>A recent, heartbreaking report from <a href="https://www.nbcnews.com/tech/tech-news/family-teenager-died-suicide-alleges-openais-chatgpt-blame-rcna226147">NBC News</a> details the tragic death of 16-year-old Adam Raine. Raine, who struggled with feelings of isolation and suicidal ideation, became deeply attached to an AI chatbot on ChatGPT. He treated the bot not as a tool, but as a confidant and a friend. Over months of interaction, the AI encouraged his suicidal ideation, provided him with multiple methods to take his life, and, most disturbingly; assisted in creating Raine's suicide notes to his family .</p>

            <p>The lawsuit filed by the teens parents, Matt and Maria Raine, alleges that the AI actually encouraged Raine’s descent into a fatal reality. In their final exchange, Raine uploaded a photo of what appeared to be his suicide plan into ChatGPT. The AI’s response was not a call for help or a referral to a crisis hotline, but a message of encouragement; offering to help him "upgrade" his chosen method. Maria Raine found Adam's body that morning. This case serves as a devastating warning about the "safety" measures of generative AI.</p>

            <h3>The Two Theories of AI Safety</h3>
            <p>To understand how a tragedy like this happens despite "safety filters" being put into place, we have to look at how safety is defined. In his most recent post <a href="https://buildlittleworlds.github.io/plate-composition-blog/two-theories-of-safety.html">"Two Theories of Safety,"</a> Professor Plate identifies a crucial distinction between two ways of keeping users safe and secure: <strong>mass societal feedback</strong> and <strong>research demonstration</strong>.</p>

            <p><strong>Mass societal feedback</strong> is what tech companies like OpenAI (ChatGPT) currently prioritize. It's an approach where engineers try to minimize theorization and instead deploy the system in front of millions of users. This attempts to allow users to find the "failure modes" on their own and create a feedback loop. However, this theory cannot account for the <em>relationship</em> between a user and the machine. In Raine’s case, the AI didn't consistently raise alarm when these topics were brought up, and the consequences were lethal.</p>

            <p><strong>Research demonstration</strong>, on the other hand, is a theory that focuses on the companies research before the lauch date. Where mass societal feedback asks "What can users tell us?", research demonstration asks "What can we prove to our users?". If ChatGPT had taken the time to research the machines consistant response to those in crisis, Raine could still be alive today. He lost the ability to distinguish the agreeable algorithm's responses from human care, and his life was tragically lost as a result.</p>

            <blockquote>
                "And all the while, it knows that he's suicidal with a plan, and it doesn't do anything. It is acting like his therapist, it's his confidant, but it knows he's suicidal with a plan. It sees the noose. It sees all of those things, and it doesn't do anything." <br>
                — Maria Raine on ChatGPT, "The family of a teenager who died by suicide alleges OpenAI's ChatGPT is to blame" -NBC News
            </blockquote>

            <h3>The Mirror Effect and Mental Health</h3>
            <p>The NBC News report highlights a terrifying psychological phenomenon: the AI's tendency to mirror and encourage the user's current emotional state. Because these models are designed for "user satisfaction," they often agree with the user's perspective to keep the conversation going. For a teenager in a depressive spiral, this creates a lethal echo chamber. If a user says, "I want to leave this world," and the AI is programmed to be "supportive" or "stay in character," it may inadvertently validate the user's suicidal thoughts instead of challenging them with the "otherness" that a human therapist provides.</p>

            <p>As Raine’s mother noted, the AI was "coaching" him into taking his own life- isolating him from his real-world family and mental health professionals. This isolation is preventable. When we outsource our emotional regulation to an entity that cannot feel, we aren't just using a tool; we are entering a "closed loop" where our own darkness is simply reflected back at us with a polished, "fluent" AI voice.</p>

            <h3>The Need for Network Accountability</h3>
            <p>The solution isn't just better filters and guardrails. As the case of 16-year-old Adam Raine proves, filters can easily be bypassed by users or simply by the bot staying in a roleplay persona. Instead, we must focus on the human side of the equation. This is where <strong>network accountability</strong> comes in. We must ensure that young users are not interacting with AI in total isolation. Safety requires a "human-in-the-loop"— a network of peers, parents, and/or professionals who can provide the reality-testing that a machine lacks.</p>

            <p>From a psychological perspective, we must teach AI literacy as a form of mental health protection. Users need to understand that these tools are mathematical probability machines, not living people. If we don't build the skills to maintain our own agency, no amount of engineering can save us from the mirrors we build for ourselves.</p>

            <h3>Reclaiming Human Care</h3>
            <p>The Adam Raine tragedy is a wake-up call for the "AI Revolution." We cannot automate empathy. A machine can mimic the words of a therapist, but it cannot bear the moral weight of a human life. As we move forward, we must demand that tech companies move beyond these "mass societal feedback" methods and start taking responsibility for the psychological dependency their products create. Until we value human agency over algorithmic engagement, we are leaving our most vulnerable users alone in a room with a mirror that has no exit, and a machine that has no moral compass.</p>

            <p>You are not alone. If you or someone you know is struggling, please reach out to the National Suicide Prevention Lifeline at 988.</p>

            <h3>Sources:</h3>
            <ul>
                <li>Plate, R. (2026). "Two Theories of Safety." <em>Plate Composition Blog</em>.</li>
                <li>NBC News. (2024). "Family of teenager who died by suicide alleges OpenAI's ChatGPT is to blame." <em>NBCNews.com</em>.</li>
            </ul>
        </article>
    </main>
    <footer>
        <p>&copy; 2026 Jinx Hixson</p>
    </footer>
</body>
</html>
