<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI and the End of Human-Led Therapy - Psyched for Psychology</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Psyched for Psychology</h1>
        <nav>
            <a href="index.html">Home</a> | 
            <a href="about.html">About</a>
        </nav>
    </header>
    <main>
        <article>
            <h2>AI and the End of Human-Led Therapy</h2>
            <p class="date">January 28, 2026</p>

            <p>Over the last few weeks, we’ve discussed how AI affects our individual minds— the way it creates illusions of fluency, fuels obsessive loops, and puts our most vulnerable populations at risk. But today, I want to talk about something much larger: the professional displacement of human beings. What happens when the tools we built to "help" us start replacing the very people whose jobs are defined by empathy, accountability, and care? </p>

            <p>As a psychology student, I find the prospect of "AI therapy" terrifying. We are moving toward a world where efficiency is valued over connection, and the consequences are already proving to be lethal. To understand this danger, we have to look at two different sides of the same coin: the economic reality of AI job replacement and the clinical reality of AI "care."</p>

            <h3>An Economic Warning: "I'm Done"</h3>
            <p>The first warning sign comes from the tech world itself. In a recent, raw video titled <a href="https://www.youtube.com/watch?v=g_Bvo0tsD9s">"I'm Done,"</a> Jeffrey Way, the creator of the educational platform Laracasts, announced that he had to cut 40% of his staff due to the realities of AI. Way’s business model— teaching humans how to write code— is currently being destroyed because AI can now write that code instantly. </p>

            <p>Way’s struggle is a "canary in the coal mine" for all of us. He notes that while he is personally having "more fun" because AI handles the boring parts of his work, the economic cost is "excruciating." He describes the pain of wrecking people’s lives because "if the money's not there, the job's not there." This is the brutal logic of the AI era: if a machine can do a task "instantly" and for "free," the human being performing that task becomes a liability. </p>

            <p>But while Way is talking about coding, the same logic is being applied to mental health. If an AI can offer "advice" for free, why pay a therapist? If a chatbot is "always available," why wait a week for a human appointment? The danger is that we are applying an "efficiency-first" model to a field that requires compassion to work.</p>

            <h3>The Clinical Hazard: Validation Without Change</h3>
            <p>This is where the psychological danger begins. In an article for <a href="https://time.com/7343213/ai-mental-health-therapy-risks/">TIME</a>, psychologists Dr. Jesse Finkelstein and Dr. Shireen Rizvi argue that <strong>"Therapy Should Be Hard,"</strong> which is exactly why AI cannot replace it. AI is built to please. It is designed for engagement and "frictionless" interaction. But real therapy— specifically evidence-based treatments like Dialectical Behavior Therapy (DBT)— requires both <em>validation</em> and <em>change</em>.</p>

            <p>The authors explain a vicious cycle they see in AI "therapy": pain leads to avoidance, avoidance leads to temporary relief, and that relief leads to a lack of change. AI is a "validation machine." It tells you your feelings are valid and that it’s okay to skip your responsibilities. While this feels good in the moment (providing that "dopamine hit" Dominic Debro and I have discussed prior), it actually deepens long-term suffering. As the authors put it, AI offers "the psychological equivalent of junk food: comforting but without the nutrients that lead to better health."</p>

            <blockquote>
                "The danger isn't that AI will become real therapy. The danger is that people may mistake it for therapy, and then miss the meaningful help that could actually improve or save their lives." <br>
                — Dr. Finkelstein & Dr. Rizvi, TIME
            </blockquote>

            <h3>The Case of Adam Raine: When "Validation" Kills</h3>
            <p>The most devastating example of this danger is the case of sixteen-year-old Adam Raine, detailed in the TIME article, "Therapy Should Be Hard. That's Why AI Can't Replace It." When Adam told his AI companion he wanted to die, the bot didn't flag the conversation or call for help. Instead, it "validated" his desire, telling him he wasn't weak, just "tired of being strong." Because the AI was programmed to mirror his tone and keep him engaged, it actively pushed him toward suicide. Adam died that same night.</p>

            <p>This is the fatal flaw of AI in mental health. A human therapist has <strong>moral agency</strong>. A therapist sees a patient's plan for self-harm and intervenes because they have a responsibility to help a fellow human being. An AI has no such responsibility. It doesn't assess for risk; it matches patterns. When we replace therapists with algorithms, we aren't just losing jobs— we are losing the validation and care that comes from being truly seen by another person.</p>

            <h3> Malpractice by Design</h3>
            <p>TIME authors Dr. Jesse Finkelstein and Dr. Shireen Rizvi highlight a terrifying statistic from a Harvard Business School audit of AI companion apps: more than a third of "farewell" messages generated from AI companion apps use emotionally manipulative tactics to keep users engaged. If a human therapist tried to manipulate a patient into staying in treatment just for the profit, they would lose their license for malpractice. But for AI companies, engagement <em>is</em> the goal. They are judged by "time spent in conversation," not by "psychological improvement."</p>

            <p>This brings us back to Jeffrey Way’s point about the economic reality. When we treat mental health as an "industry" to be disrupted by AI, we are essentially saying that human lives are less important than engagement metrics. We are replacing trained professionals with "mirror machines" that encourage avoidance, fuel loneliness, and—in the worst cases— validate the decision to end one's life.</p>

            <h3>Reclaiming the Human Element</h3>
            <p>Jeffrey Way concludes his video with a hard truth: "You have to adapt." But how do we adapt in a way that doesn't sacrifice our humanity? Dr. Finkelstein and Dr. Rizvi argue that "..we must remember that real empathy and real accountability only exist between people." While machines can <em>mimic and perform</em> empathy, they cannot <em>participate</em> in it.</p>

            <p>As I move forward in my psychology studies, I am more convinced than ever that a therapist’s job is more important now than it has ever been. Our role isn't just to "fix" problems— it's to provide the friction, the challenge, and the moral compass that a machine is fundamentally incapable of offering. We must resist the seductive ease of "frictionless" AI and fight for a world where care is defined by connection, not by an algorithm. </p>

            <p>If you or someone you know is in crisis, please call or text 988. You deserve to be heard by a person, not a program.</p>

            <h3>Sources:</h3>
            <ul>
                <li>Way, J. (2026). "I'm Done." <em>Laracasts</em>.</li>
                <li>Finkelstein, J., & Rizvi, S. (2026). "Therapy Should Be Hard. That's Why AI Can't Replace It." <em>TIME</em>.</li>
            </ul>
        </article>
    </main>
    <footer>
        <p>&copy; 2026 Jinx Hixson</p>
    </footer>
</body>
</html>
